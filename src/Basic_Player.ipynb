{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Minesweeper import Minesweeper\n",
    "from MinesweeperGUI import MinesweeperGUI\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import pygame\n",
    "\n",
    "from collections import deque\n",
    "import copy\n",
    "import typing\n",
    "import random\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Cell for doing some safety checking on the users directory structure. \n",
    "    This code should not be executed outside of its expected location. Modify the cell below at your own risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_files = os.listdir(\"..\")\n",
    "\n",
    "auth = False\n",
    "\n",
    "if \"execution.key\" in root_files:\n",
    "    with open(\"../execution.key\", \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            else:\n",
    "                if line == \"ff7f6519d0f9fb3d1eef5fac3fec7e83\":\n",
    "                    auth = True\n",
    "\n",
    "if not auth:\n",
    "    raise RuntimeError(\n",
    "        \"Execution is not occuring in the correct directory structure.\"\n",
    "        + \"\\nPlease re-pull from GitHub:\"\n",
    "        + \" https://github.com/EkinsMatthew/MinesweeperDQN\"\n",
    "    )\n",
    "\n",
    "else:\n",
    "    if \"outputs\" not in root_files:\n",
    "        os.mkdir(\"../outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = typing.TypeVar(\"T\")\n",
    "\n",
    "class Deck(typing.Generic[T]):\n",
    "    def __init__(self, max_size: int):\n",
    "        self.queue: deque[T] = deque()\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def append(self, obs: T) -> None:\n",
    "        if len(self.queue) >= self.max_size:\n",
    "            self.queue.popleft()\n",
    "        self.queue.append(obs)\n",
    "\n",
    "    def sample(self, n: int) -> list[T]:\n",
    "        return random.sample(self.queue, n)\n",
    "\n",
    "    def mean(self) -> float:\n",
    "        return sum(self.queue) / len(self.queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicAgent(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: int,\n",
    "        output_shape: int,\n",
    "        relu_slope: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_shape, 128),\n",
    "            nn.LeakyReLU(relu_slope),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(relu_slope),\n",
    "            nn.Linear(64, output_shape),\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        return self.model(x.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MinesweeperDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(MinesweeperDQN, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=16,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=16,\n",
    "            out_channels=32,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "\n",
    "        # Calculate the size after convolution (for fully connected input)\n",
    "        conv_output_size = (\n",
    "            32 * input_shape[0] * input_shape[1]\n",
    "        )  # 32 channels after conv2\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(conv_output_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        # Convolutional layers\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "\n",
    "        # Flatten the tensor before fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "\n",
    "        # Output layer (action space)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinesweeperDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(MinesweeperDQN, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=16,\n",
    "            kernel_size=4,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=16,\n",
    "            out_channels=32,\n",
    "            kernel_size=4,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "\n",
    "        # Calculate the size after convolution (for fully connected input)\n",
    "        # This is now fixed to reflect the output size of conv2\n",
    "        conv_output_size = 4 * 4 * 32\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(conv_output_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, input_shape[0] * input_shape[0])\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        # Convolutional layers\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "\n",
    "        # Flatten the tensor before fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "\n",
    "        # Output layer (action space)\n",
    "        x = self.fc3(x).flatten()\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4*4*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Manager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        difficulty: typing.Literal[\"EZ\", \"B\", \"I\", \"E\"],\n",
    "        action_set_size: typing.Literal[1, 2, 3],\n",
    "        gamma: float,\n",
    "        memory_length: int,\n",
    "        learning_rate: float,\n",
    "        momentum: float,\n",
    "        use_gui: bool,\n",
    "        output_dir: str,\n",
    "    ):\n",
    "        \"\"\"An object that monitors and manages the training of a MinesweeperAI.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        difficulty : {\"B\", \"I\", \"E\"}\n",
    "            Which classic Minesweeper difficulty rules (board size and mine\n",
    "            count) should be used to create the backend game board? \"B\" for\n",
    "            Beginner, \"I\" for Intermediate, and \"E\" for Expert.\n",
    "        action_set_size : {1, 2, 3}\n",
    "            What is the action set size for the player? There are three options:\n",
    "            1 for a game where the player can only left click; 2 for a game\n",
    "            where the player can left click to expose squares and also flag\n",
    "            squares as mines; 3 for a game where the player can expose, flag,\n",
    "            and also use the test flag function (traditionaly left and right\n",
    "            click simultaneously)\n",
    "        gamma : float\n",
    "            _description_\n",
    "        memory_length : int\n",
    "            How long should the memory set of the experience replayer be before\n",
    "            past examples are deleted from the set?\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If the user passes an illegal value for the difficulty argument\n",
    "        \"\"\"\n",
    "        self.difficulty = difficulty\n",
    "\n",
    "        if self.difficulty == \"EZ\":\n",
    "            x = 6\n",
    "            y = 6\n",
    "            mines = 4\n",
    "        elif self.difficulty == \"B\":\n",
    "            x = 9\n",
    "            y = 9\n",
    "            mines = 10\n",
    "        elif self.difficulty == \"I\":\n",
    "            x = 16\n",
    "            y = 16\n",
    "            mines = 40\n",
    "        elif self.difficulty == \"E\":\n",
    "            x = 30\n",
    "            y = 16\n",
    "            mines = 99\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Difficulty must be one of three values (EZ, B, I, E): {difficulty}\"\n",
    "            )\n",
    "\n",
    "        self.game = Minesweeper()\n",
    "        self.game.initialize_game_state(x, y, mines)\n",
    "\n",
    "        self.use_gui = use_gui\n",
    "\n",
    "        if self.use_gui:\n",
    "            self.gui = MinesweeperGUI(\n",
    "                self.game,\n",
    "                zoom_factor=10,\n",
    "                tile_set_number=2,\n",
    "            )\n",
    "\n",
    "        # The number of actions that the deep learning player can make\n",
    "        self.ACTION_SET_SIZE = action_set_size\n",
    "        # The action vector is the size of the board multiplied by the number of\n",
    "        # actions that are possible to take\n",
    "        self.ACTION_VECTOR_SIZE = x * y * self.ACTION_SET_SIZE\n",
    "        # The index numbers of the various actions our model can take\n",
    "        self.ACTION_FUNCTIONS = {\n",
    "            0: self.game.discover_tile,\n",
    "            1: self.game.flag_tile,\n",
    "            2: self.game.test_number_tile,\n",
    "        }\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.start_game(x, y, mines)\n",
    "\n",
    "        self.online = MinesweeperDQN(\n",
    "            input_shape=(self.game.x, self.game.y),\n",
    "            num_actions=action_set_size,\n",
    "        ).to(self.game.device)\n",
    "\n",
    "        self.target = MinesweeperDQN(\n",
    "            input_shape=(self.game.x, self.game.y),\n",
    "            num_actions=action_set_size,\n",
    "        ).to(self.game.device)\n",
    "\n",
    "        # Initialize the models together in an identical form\n",
    "        self.target.load_state_dict(self.online.state_dict())\n",
    "\n",
    "        # self.online = BasicAgent(\n",
    "        #     input_shape=self.game.x * self.game.y,\n",
    "        #     output_shape=self.game.x * self.game.y * self.ACTION_SET_SIZE,\n",
    "        #     relu_slope=0.2,\n",
    "        # ).to(self.game.device)\n",
    "\n",
    "        # self.target = copy.deepcopy(self.online)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.optimizer = torch.optim.RMSprop(\n",
    "            self.online.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            momentum=self.momentum,\n",
    "        )\n",
    "\n",
    "        self.total_loss: list[torch.Tensor] = []\n",
    "        self.running_reward: list[float] = []\n",
    "        self.rewards: list[float] = []\n",
    "\n",
    "        self.q_preds: list[float] = []\n",
    "        self.q_targets: list[float] = []\n",
    "\n",
    "        self.q_spread: list[tuple[torch.Tensor, torch.Tensor]] = []\n",
    "\n",
    "        # Queue to store all of the state action pairs, the reward of that\n",
    "        # action, as well as the next resultant state and next action\n",
    "        self.experience_replayer: Deck[\n",
    "            tuple[\n",
    "                # State\n",
    "                torch.Tensor,\n",
    "                # Action\n",
    "                int,\n",
    "                # Reward\n",
    "                float,\n",
    "                # Last state terminality\n",
    "                bool,\n",
    "                # Result State\n",
    "                torch.Tensor,\n",
    "            ]\n",
    "        ] = Deck(max_size=memory_length)\n",
    "\n",
    "        # Some pointers to assist with constructing the experience replayer\n",
    "        self.last_state: torch.Tensor = torch.empty(0, device=self.game.device)\n",
    "        self.last_action: int\n",
    "        self.last_action_reward: float\n",
    "\n",
    "        # Counter for the number of actions taken\n",
    "        self.steps: int = 0\n",
    "\n",
    "        # The location where general outputs will be written to\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def start_game(\n",
    "        self,\n",
    "        x: int,\n",
    "        y: int,\n",
    "        mines: int | None = None,\n",
    "    ):\n",
    "        \"\"\"Initialize the internal minesweeper game that will be trained on.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : int\n",
    "            The horizontal size of the game board.\n",
    "        y : int\n",
    "            The vertical size of the game board.\n",
    "        mines : int | None\n",
    "            The number of mines to place on the game board.\n",
    "        \"\"\"\n",
    "        self.game.initialize_game_state(x, y, mines)\n",
    "\n",
    "    def restart_game(self):\n",
    "        \"\"\"Reset the internal game when necessary.\"\"\"\n",
    "        self.game.reinitialize_game_state()\n",
    "\n",
    "    def process_move(\n",
    "        self,\n",
    "        action_number: int,\n",
    "        action_x: int,\n",
    "        action_y: int,\n",
    "    ) -> tuple[float, bool]:\n",
    "        \"\"\"Take an action coded as an action type and location, perform that\n",
    "        action on the game board, and determine the reward to give to our\n",
    "        internal model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action_number : int\n",
    "            The number that corresponds to the action type that should be\n",
    "            executed; see action functions\n",
    "        action_x : int\n",
    "            _description_\n",
    "        action_y : int\n",
    "            _description_\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple[float, bool]\n",
    "            _description_\n",
    "        \"\"\"\n",
    "        # Given our action and coordinate\n",
    "        result = self.ACTION_FUNCTIONS[action_number](action_x, action_y)\n",
    "\n",
    "        # If the move was illegal\n",
    "        if not result:\n",
    "            reward = -5\n",
    "            terminal = False\n",
    "        #  or if flagging, no explicit reward\n",
    "        if action_number == 1:\n",
    "            reward = 0\n",
    "            terminal = False\n",
    "\n",
    "        # If the previous was a terminal state\n",
    "        if self.game.over:\n",
    "            if self.game.lost:\n",
    "                reward = -2 * self.game.num_tiles\n",
    "            else:\n",
    "                reward = 2 * self.game.num_tiles\n",
    "            terminal = True\n",
    "\n",
    "        else:\n",
    "            # Else, there was a legal discovery performed that did not lose the game\n",
    "            discovery_rate = self.game.num_discovered / (\n",
    "                self.game.num_tiles - self.game.num_mines\n",
    "            )\n",
    "\n",
    "            # Linearly increase the reward based on the number of tiles the online\n",
    "            # agent has discovered\n",
    "            reward = 1 + discovery_rate * 1\n",
    "            terminal = False\n",
    "\n",
    "        if terminal:\n",
    "            self.gui.refresh()\n",
    "            self.restart_game()\n",
    "\n",
    "        return reward, terminal\n",
    "\n",
    "    def poll_model(self) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        online_Qs: torch.Tensor = self.online(self.game.board)\n",
    "        target_Qs: torch.Tensor = self.target(self.game.board)\n",
    "\n",
    "        self.q_spread.append(\n",
    "            (\n",
    "                online_Qs.min().detach().cpu(),\n",
    "                online_Qs.max().detach().cpu(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return online_Qs, target_Qs\n",
    "\n",
    "    def step(self, epsilon: float):\n",
    "        # Poll the model for an action\n",
    "        online_Qs, _ = self.poll_model()\n",
    "\n",
    "        # Random interger to determine if we take a random action\n",
    "        n = random.uniform(0, 1)\n",
    "\n",
    "        # Take a random action\n",
    "        if n < epsilon:\n",
    "            # Select all parts of the action space randomly\n",
    "            action_number = random.randint(0, self.ACTION_SET_SIZE - 1)\n",
    "            action_x = random.randint(0, self.game.x - 1)\n",
    "            action_y = random.randint(0, self.game.y - 1)\n",
    "\n",
    "            # Calculate the location in the tensor that this action would have\n",
    "            # come from\n",
    "            action_loc = action_y * self.ACTION_SET_SIZE + action_x\n",
    "\n",
    "        # Take an online-chosen action\n",
    "        else:\n",
    "            # Get the location of the maximally ranked Q value\n",
    "            action_loc = int(torch.argmax(online_Qs))\n",
    "\n",
    "            # Given where that Q is, what action does that imply?\n",
    "            action_number = action_loc % self.ACTION_SET_SIZE\n",
    "\n",
    "            # Ignoring the number of actions, which tile are we acting on?\n",
    "            normalized_action_loc = action_loc // self.ACTION_SET_SIZE\n",
    "\n",
    "            # X and Y coordinate of that tile\n",
    "            action_x = normalized_action_loc % self.game.y\n",
    "            action_y = normalized_action_loc // self.game.x\n",
    "\n",
    "        # Reward for this action\n",
    "        reward, terminal = self.process_move(\n",
    "            action_number,\n",
    "            action_x,\n",
    "            action_y,\n",
    "        )\n",
    "\n",
    "        current_board = self.game.board.clone()\n",
    "\n",
    "        # After the first move, start building the experience replayer\n",
    "        if self.steps > 0:\n",
    "            # Add the tuple set of (s_t, a_t, r_t, s_t+1)\n",
    "            self.experience_replayer.append(\n",
    "                (\n",
    "                    self.last_state,\n",
    "                    self.last_action,\n",
    "                    self.last_action_reward,\n",
    "                    self.last_action_terminal,\n",
    "                    current_board,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Store the state of this game board now so that we can use it for\n",
    "        # saving states to the experience_replayer\n",
    "        self.last_state = current_board\n",
    "        self.last_action = action_loc\n",
    "        self.last_action_reward = reward\n",
    "        self.last_action_terminal = terminal\n",
    "\n",
    "        self.running_reward.append(reward)\n",
    "\n",
    "        self.steps += 1\n",
    "\n",
    "    def reset_target(self):\n",
    "        \"\"\"Function for updating the online model to match the target over time\"\"\"\n",
    "        self.target.load_state_dict(self.online.state_dict())\n",
    "\n",
    "    def play(\n",
    "        self, M: int, T: int, epsilon_target: float, batch_size: int, training_frequency\n",
    "    ):\n",
    "        epsilon = 1.0\n",
    "        for m in range(M):\n",
    "            for t in range(T):\n",
    "                previous_steps = t + m * T\n",
    "\n",
    "                # Fix epsilon at zero for the first episode\n",
    "                if m == 0:\n",
    "                    epsilon = 1.0\n",
    "\n",
    "                elif (m > 0) & (m < 5):\n",
    "                    # Linear annealing of epsilon over the second two episodes\n",
    "                    epsilon = 1 - (1 - epsilon_target) * (\n",
    "                        (previous_steps - T) / (4 * T)\n",
    "                    )\n",
    "\n",
    "                elif m >= 5:\n",
    "                    epsilon = epsilon_target\n",
    "\n",
    "                self.step(epsilon=epsilon)\n",
    "\n",
    "                print(\n",
    "                    f\"t: {t} of Episode: {m} Epsilon: {round(epsilon, 3)}\\r\",\n",
    "                    end=\"\",\n",
    "                )\n",
    "\n",
    "                if (self.steps > batch_size) & (self.steps % training_frequency == 0):\n",
    "                    # print(f\"\\n\\tTraining: \")\n",
    "\n",
    "                    training_round_loss: list[torch.Tensor] = []\n",
    "                    training_examples = self.experience_replayer.sample(batch_size)\n",
    "\n",
    "                    for te in training_examples:\n",
    "                        # Expected reward as defined by the max Q value of our\n",
    "                        # online network\n",
    "                        s_i = te[0]  # Current state\n",
    "                        a_i = te[1]  # Action taken to get here\n",
    "                        r_i = te[2]  # Reward for that action\n",
    "                        terminal_state = te[3]  # was it a terminal action\n",
    "                        s_i_prime = te[4]  # Resultant state from action\n",
    "\n",
    "                        # What does the target say about this action?\n",
    "                        target_Q_i = self.target.forward(\n",
    "                            s_i\n",
    "                        ).detach()  # \"Detach from torch graph so it doesn't\n",
    "                        # mess up gradient calculation.\" -E. Crouse\n",
    "                        # https://github.com/ImagineOrange/Deep-RL-Paper-Implementations/blob/main/CHEEMS_DDDQN.py\n",
    "                        # Line 711\n",
    "\n",
    "                        # What does the target say about the expected reward of the next step?\n",
    "                        target_Q_i_prime = self.target.forward(s_i_prime).detach()\n",
    "\n",
    "                        # If we need to take into acount the future reward\n",
    "                        if not terminal_state:\n",
    "                            y_i = r_i + (\n",
    "                                self.gamma\n",
    "                                * target_Q_i_prime[int(torch.argmax(target_Q_i_prime))]\n",
    "                                # self.gamma\n",
    "                                # * target_Q_i[a_i]\n",
    "                            )\n",
    "                            self.q_targets.append(y_i.detach().cpu())\n",
    "                        else:\n",
    "                            y_i = r_i\n",
    "                            self.q_targets.append(y_i)\n",
    "\n",
    "                        q_pred = self.online.forward(s_i)[a_i]\n",
    "\n",
    "                        self.q_preds.append(q_pred.detach().cpu())\n",
    "\n",
    "                        loss = (q_pred - y_i) ** 2\n",
    "\n",
    "                        # print(loss)\n",
    "\n",
    "                        self.optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        self.optimizer.step()\n",
    "                        training_round_loss.append(loss.detach().cpu())\n",
    "\n",
    "                    average_training_loss = sum(training_round_loss) / len(\n",
    "                        training_round_loss\n",
    "                    )\n",
    "\n",
    "                    # print(f\"\\t\\tLoss: {average_training_loss}\")\n",
    "                    self.total_loss.append(average_training_loss)\n",
    "                    # print(f\"\\t\\tRunning Reward: {round(self.running_reward.mean(), 3)}\")\n",
    "                    self.rewards.append(\n",
    "                        sum(self.running_reward) / len(self.running_reward)\n",
    "                    )\n",
    "                    self.running_reward = []\n",
    "\n",
    "                if self.steps % 500 == 0:\n",
    "                    self.reset_target()\n",
    "                    print(\n",
    "                        f\"\\n\\t\\t Target Reset\"\n",
    "                        + f\"\\n\\t\\t\\tAverage Loss since last update: {sum(self.total_loss[-10:])/10}\"\n",
    "                        + f\"\\n\\t\\t\\tAverage Reward since last update: {sum(self.rewards[-10:])/10}\"\n",
    "                    )\n",
    "\n",
    "                if self.use_gui:\n",
    "                    self.gui.refresh()\n",
    "                    pygame.event.wait(timeout=1)\n",
    "\n",
    "            # Save the models at the end of each episode\n",
    "            torch.save(\n",
    "                self.online.state_dict(),\n",
    "                f\"{self.output_dir}/online_model_episode_{m}.pth\",\n",
    "            )\n",
    "            torch.save(\n",
    "                self.target.state_dict(),\n",
    "                f\"{self.output_dir}/target_model_episode_{m}.pth\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_start_time = datetime.datetime.now().strftime(r\"%Y-%m-%d--%H-%M-%S\")\n",
    "output_dir = f\"../outputs/{execution_start_time}\"\n",
    "\n",
    "os.mkdir(output_dir)\n",
    "\n",
    "paramters = {\n",
    "    \"difficulty\": \"EZ\",\n",
    "    \"action_set_size\": 1,\n",
    "    \"gamma\": 0.9,\n",
    "    \"memory_length\": int(1e6),\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"momentum\": 0.0,\n",
    "    \"use_gui\": True,\n",
    "    \"M\": 10,\n",
    "    \"T\": int(5e5),\n",
    "}\n",
    "\n",
    "with open(f\"./outputs/{execution_start_time}/model_parameters.JSON\", \"w\") as f:\n",
    "    f.write(paramters.__str__())\n",
    "\n",
    "agent = Manager(\n",
    "    difficulty=paramters[\"difficulty\"],\n",
    "    action_set_size=paramters[\"action_set_size\"],\n",
    "    gamma=paramters[\"gamma\"],\n",
    "    memory_length=paramters[\"memory_length\"],\n",
    "    learning_rate=paramters[\"learning_rate\"],\n",
    "    momentum=paramters[\"momentum\"],\n",
    "    use_gui=paramters[\"use_gui\"],\n",
    "    output_dir=output_dir,\n",
    ")\n",
    "\n",
    "agent.play(\n",
    "    M=paramters[\"M\"],\n",
    "    T=paramters[\"T\"],\n",
    "    epsilon_target=0.05,\n",
    "    batch_size=64,\n",
    "    training_frequency=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10\n",
    "T = 100\n",
    "epsilon_target = 0.3\n",
    "\n",
    "for m in range(M):\n",
    "    for t in range(T):\n",
    "        previous_steps = t + m * T\n",
    "\n",
    "        # Fix epsilon at zero for the first episode\n",
    "        if m == 0:\n",
    "            epsilon = 1.0\n",
    "\n",
    "        elif (m > 0) & (m < 3):\n",
    "            # Linear annealing of epsilon over the second two episodes\n",
    "            epsilon = 1 - (1 - epsilon_target) * (\n",
    "                (previous_steps - T) / (2 * T)\n",
    "            )\n",
    "\n",
    "        elif m >= 3:\n",
    "            epsilon = epsilon_target\n",
    "\n",
    "        print(m, t, previous_steps, round(epsilon, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_average_w_window(data, window_size: int, data_name: str):\n",
    "    # Calculate the moving average of the data\n",
    "    averages = np.convolve(data, np.ones(window_size) / window_size, mode=\"valid\")\n",
    "\n",
    "    # Create the x-axis values corresponding to the averages\n",
    "    x_values = np.arange(window_size - 1, len(data))\n",
    "\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot the moving average\n",
    "    ax.plot(\n",
    "        x_values,\n",
    "        averages,\n",
    "        label=f\"Moving Average (window size={window_size})\",\n",
    "        color=\"blue\",\n",
    "    )\n",
    "\n",
    "    # Set title and labels\n",
    "    ax.set_title(f\"Average {data_name} Over Time\")\n",
    "    ax.set_xlabel(\"Training Step\")\n",
    "    ax.xaxis.set_major_formatter(\n",
    "        ticker.FuncFormatter(lambda x, y: f\"{x * window_size:,.0f}\")\n",
    "    )\n",
    "\n",
    "    ax.set_ylabel(f\"Average {data_name}\")\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_average_w_window(agent.total_loss, window_size=1000, data_name=\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_average_w_window(agent.rewards, window_size=1000, data_name=\"Reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mins = [tup[0] for tup in agent.q_spread]\n",
    "maxes = [tup[1] for tup in agent.q_spread]\n",
    "\n",
    "spread = [tup[1] - tup[0] for tup in agent.q_spread]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_average_w_window(spread, window_size=10000, data_name=\"Raw Q Spread\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_average_w_window(spread, window_size=100000, data_name=\"Raw Q Spread\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.q_spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_q_spread(data_tuples, window_size: int, data_name: str):\n",
    "    # Split the list of tuples into two separate lists\n",
    "    first_entries = [tup[0] for tup in data_tuples]\n",
    "    second_entries = [tup[1] for tup in data_tuples]\n",
    "\n",
    "    # Calculate the moving averages for both first and second entries\n",
    "    downsampled_mins = first_entries[::window_size]\n",
    "    downsampled_maxes = second_entries[::window_size]\n",
    "\n",
    "    # Create the x-axis values corresponding to the averages\n",
    "    x_values = np.arange(0, len(downsampled_mins))\n",
    "\n",
    "    # Create a figure and axis\n",
    "    ax: plt.Axes\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot the moving average for the first entries\n",
    "    ax.plot(\n",
    "        x_values,\n",
    "        downsampled_mins,\n",
    "        label=f\"Q Min (window size={window_size})\",\n",
    "        color=\"blue\",\n",
    "    )\n",
    "\n",
    "    # Plot the moving average for the second entries\n",
    "    ax.plot(\n",
    "        x_values,\n",
    "        downsampled_maxes,\n",
    "        label=f\"Q Max (window size={window_size})\",\n",
    "        color=\"green\",\n",
    "    )\n",
    "\n",
    "    # Set title and labels\n",
    "    ax.set_title(f\"Average {data_name} Over Time\")\n",
    "    ax.set_xlabel(\"Training Step\")\n",
    "    ax.xaxis.set_major_formatter(\n",
    "        ticker.FuncFormatter(lambda x, y: f\"{int(x * window_size)}\")\n",
    "    )\n",
    "\n",
    "    ax.set_ylabel(f\"Average {data_name}\")\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_q_spread(agent.q_spread, window_size=1000, data_name=\"Raw Q\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    time.sleep(0.1)\n",
    "    agent.step(epsilon=0)\n",
    "    agent.gui.refresh()\n",
    "    pygame.event.wait(timeout=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.gui.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.game.discover_tile(3, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_Qs = agent.online.forward(agent.game.board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the location of the maximally ranked Q value\n",
    "action_loc = int(torch.argmax(online_Qs))\n",
    "\n",
    "# Given where that Q is, what action does that imply?\n",
    "action_number = action_loc % agent.ACTION_SET_SIZE\n",
    "\n",
    "# Ignoring the number of actions, which tile are we acting on?\n",
    "normalized_action_loc = action_loc // agent.ACTION_SET_SIZE\n",
    "\n",
    "# X and Y coordinate of that tile\n",
    "action_x = normalized_action_loc % agent.game.y\n",
    "action_y = normalized_action_loc // agent.game.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.game.board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.game.board.transpose(-1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.q_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_targets = []\n",
    "\n",
    "for q_target in agent.q_targets:\n",
    "    if type(q_target) is int:\n",
    "        q_targets.append(q_target)\n",
    "    else:\n",
    "        q_targets.append(q_target.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(100, 20))\n",
    "\n",
    "x_axis = np.arange(0, len(agent.q_targets))\n",
    "\n",
    "ax.scatter(\n",
    "    x_axis,\n",
    "    agent.q_preds,\n",
    "    label=\"Q Pred\",\n",
    "    s=1,\n",
    "    alpha=0.7,\n",
    ")\n",
    "ax.scatter(\n",
    "    x_axis,\n",
    "    agent.q_targets,\n",
    "    label=\"Q Target\",\n",
    "    s=1,\n",
    "    alpha=0.7,\n",
    ")\n",
    "\n",
    "# Set title and labels\n",
    "ax.set_title(\"Q Values over Training Epochs\")\n",
    "ax.set_xlabel(\"Training Example\")\n",
    "ax.set_ylabel(\"Q Value\")\n",
    "ax.legend()\n",
    "# ax.grid()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(30, 18))\n",
    "\n",
    "sample_rate = 1000\n",
    "\n",
    "q_preds_sampled = agent.q_preds[::sample_rate]\n",
    "q_targets_sampled = agent.q_targets[::sample_rate]\n",
    "\n",
    "x_axis = np.arange(0, len(q_targets_sampled))\n",
    "\n",
    "ax.scatter(\n",
    "    x_axis,\n",
    "    q_preds_sampled,\n",
    "    label=\"Q Pred\",\n",
    "    s=1,\n",
    "    alpha=0.7,\n",
    ")\n",
    "ax.scatter(\n",
    "    x_axis,\n",
    "    q_targets_sampled,\n",
    "    label=\"Q Target\",\n",
    "    s=1,\n",
    "    alpha=0.7,\n",
    ")\n",
    "\n",
    "# Set title and labels\n",
    "ax.set_title(\"Q Values over Training Epochs\")\n",
    "ax.set_xlabel(f\"Every {sample_rate}th Training Example\")\n",
    "ax.set_ylabel(\"Q Value\")\n",
    "ax.legend()\n",
    "# ax.grid()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(q_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agent.total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agent.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "\n",
    "repetitions = 1000000\n",
    "\n",
    "start_time = time.time()\n",
    "dictionary_list = []\n",
    "for _ in range(repetitions):\n",
    "    dictionary_data = {k: random.random() for k in range(30)}\n",
    "    dictionary_list.append(dictionary_data)\n",
    "end_time = time.time()\n",
    "print('Execution time for generation [list of dict (row store)] = %.6f seconds' % (end_time-start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "df_final1 = pd.DataFrame.from_dict(dictionary_list)\n",
    "end_time = time.time()\n",
    "print('Execution time for conversion to pandas [list of dict (row store)] = %.6f seconds' % (end_time-start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "list_dictionnary = {k: [] for k in range(30)}\n",
    "for k in range(30):\n",
    "    for _ in range(repetitions):\n",
    "        list_dictionnary[k].append(random.random())\n",
    "end_time = time.time()\n",
    "print('Execution time for generation [dict of list (column store) = %.6f seconds' % (end_time-start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "df_final2 = pd.DataFrame(list_dictionnary)\n",
    "end_time = time.time()\n",
    "print('Execution time for conversion to pandas [dict of list (column store)] = %.6f seconds' % (end_time-start_time))\n",
    "\n",
    "print(df_final1.shape)\n",
    "print(df_final2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agent.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([x.cpu() for x in agent.total_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent.online.forward(agent.game.board.flatten())\n",
    "\n",
    "output[torch.argmax(output)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.game.board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(agent.experience_replayer.queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.experience_replayer.queue[997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for replay in agent.experience_replayer.queue:\n",
    "    print(replay[1][0], replay[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.display.flip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.gui.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.discover_tile(8, 8)\n",
    "pygame.event.wait()\n",
    "gui.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.board.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Models\n",
    "\n",
    "board_size = (10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.flag_tile(20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = torch.zeros([3, 3], dtype=torch.int8)\n",
    "mask = torch.tensor([[1, 0, 1], [0, 1, 0], [0, 0, 1]])\n",
    "data = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(torch.argmax(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.where(mask == 1, data, template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template.masked_fill(mask, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for coord in torch.nonzero(mask, as_tuple=False):\n",
    "    print(coord[0], coord[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mask == 1) & (data == 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 10\n",
    "y = x\n",
    "\n",
    "[(i, j) for i in range(x) for j in range(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.where(mask == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
